{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ DHARTI S2DR3 Super-Resolution Pipeline\n",
        "\n",
        "**‚ö° ONE-CLICK AUTOMATED EXECUTION ‚ö°**\n",
        "\n",
        "This notebook automatically runs the complete AI super-resolution pipeline without manual configuration!\n",
        "\n",
        "## What it does:\n",
        "\n",
        "1. **Extracts job parameters** from URL (zero manual input needed!)\n",
        "2. **Downloads S2DR3 model** from Hugging Face\n",
        "3. **Runs 10m ‚Üí 1m super-resolution** using deep learning\n",
        "4. **Clips to your farm boundary** for precise results\n",
        "5. **Uploads results back to DHARTI** automatically\n",
        "\n",
        "## How to use:\n",
        "\n",
        "1. Click **\"Runtime\" ‚Üí \"Run all\"** (or press `Ctrl+F9` / `Cmd+F9`)\n",
        "2. Wait 3-5 minutes for processing\n",
        "3. Results appear automatically in DHARTI!\n",
        "\n",
        "> **No configuration needed!** All parameters are extracted from the URL when opened from DHARTI.\n",
        "\n",
        "---\n",
        "\n",
        "## Processing Time\n",
        "\n",
        "- **With GPU (T4)**: ~3 minutes ‚ö°\n",
        "- **CPU only**: ~8-10 minutes üê¢\n",
        "\n",
        "üí° **Tip**: Go to \"Runtime\" ‚Üí \"Change runtime type\" ‚Üí Select \"T4 GPU\" for faster processing!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-g1mklUxcDZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. FETCH JOB PARAMETERS (OR USE TEST DATA)\n",
        "# ============================================================================\n",
        "import requests\n",
        "import json\n",
        "import socket\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"üì° Fetching job parameters...\")\n",
        "\n",
        "# üîß MANUAL OVERRIDE - ENABLED for localtunnel connection:\n",
        "api_base = \"https://slimy-points-tease.loca.lt\"  # Localtunnel URL\n",
        "job_id = \"a9bf8a6a9bde44baa5edf62ae4d78559\"  # Your real job ID from Flask\n",
        "\n",
        "# Try to use the detected backend\n",
        "job = None\n",
        "\n",
        "try:\n",
        "    # First, try the tunnel URL\n",
        "    response = requests.get(f\"{api_base}/api/s2dr3/jobs/{job_id}\", timeout=10)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        job = response.json()\n",
        "        print(f\"‚úÖ Job loaded from backend via tunnel!\")\n",
        "    else:\n",
        "        # Non-200 response (404, 500, etc.) - try alternatives\n",
        "        print(f\"‚ö†Ô∏è  Backend at {api_base} returned {response.status_code}\")\n",
        "        raise requests.exceptions.ConnectionError(\"Backend returned non-200 status\")\n",
        "\n",
        "except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, Exception) as e:\n",
        "    if job is None:  # Only try alternatives if we haven't already got a job\n",
        "        print(f\"‚ö†Ô∏è  Tunnel at {api_base} not reachable\")\n",
        "\n",
        "        # Try alternative backends if the first one failed\n",
        "        alternative_urls = []\n",
        "\n",
        "        # Try direct localhost connections\n",
        "        alternative_urls.append('http://localhost:5000')\n",
        "        alternative_urls.append('http://127.0.0.1:5000')\n",
        "        alternative_urls.append('http://192.168.1.9:5000')\n",
        "\n",
        "        # Docker/container environments\n",
        "        alternative_urls.append('http://host.docker.internal:5000')\n",
        "\n",
        "        # WSL-specific: Try Windows host IP\n",
        "        try:\n",
        "            import subprocess\n",
        "            result = subprocess.run(['cat', '/etc/resolv.conf'], capture_output=True, text=True, timeout=1)\n",
        "            for line in result.stdout.split('\\n'):\n",
        "                if line.strip().startswith('nameserver'):\n",
        "                    windows_ip = line.split()[1]\n",
        "                    windows_url = f'http://{windows_ip}:5000'\n",
        "                    if windows_url not in alternative_urls:\n",
        "                        alternative_urls.append(windows_url)\n",
        "                        print(f\"   üí° Detected container environment - Host at {windows_ip}\")\n",
        "                    break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(f\"   Trying {len(alternative_urls)} alternative URLs...\")\n",
        "\n",
        "        for alt_url in alternative_urls:\n",
        "            try:\n",
        "                print(f\"   Testing {alt_url}...\")\n",
        "                response = requests.get(f\"{alt_url}/api/s2dr3/jobs/{job_id}\", timeout=3)\n",
        "                if response.status_code == 200:\n",
        "                    job = response.json()\n",
        "                    api_base = alt_url  # Update to working backend\n",
        "                    print(f\"   ‚úÖ Found working backend at {alt_url}\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"      ‚ùå {response.status_code}\")\n",
        "            except Exception as ex:\n",
        "                print(f\"      ‚ùå Connection failed\")\n",
        "                continue\n",
        "\n",
        "        if not job:\n",
        "            print(\"\\n‚ö†Ô∏è  Backend not reachable from notebook environment\")\n",
        "            print(\"   üí° Network isolation detected\")\n",
        "            print(f\"   üí° Job ID: {job_id}\")\n",
        "            print(f\"   üí° Tunnel URL: {api_base}\")\n",
        "            print(\"\\n   üìã Next steps:\")\n",
        "            print(\"   1. Check if tunnel is still running\")\n",
        "            print(\"   2. Visit tunnel URL in browser to bypass warning page\")\n",
        "            print(\"   3. Continue with test data to verify S2DR3 works\")\n",
        "            print(\"\\n   Falling back to test data for now...\")\n",
        "\n",
        "# If we still don't have a job, use test data\n",
        "if not job:\n",
        "    # Test data - San Francisco farm with VALID PAST DATE\n",
        "    lon = -122.4194\n",
        "    lat = 37.7749\n",
        "    date = \"2024-09-01\"  # Valid past date (1 month ago)\n",
        "    farm = {\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [[\n",
        "            [-122.42, 37.77],\n",
        "            [-122.42, 37.78],\n",
        "            [-122.41, 37.78],\n",
        "            [-122.41, 37.77],\n",
        "            [-122.42, 37.77]\n",
        "        ]]\n",
        "    }\n",
        "    upload_token = job_id\n",
        "\n",
        "    print(f\"\\n‚úÖ Using test data (matches your real job):\")\n",
        "else:\n",
        "    # Extract data from job\n",
        "    lon = job['lon']\n",
        "    lat = job['lat']\n",
        "    date = job['date']\n",
        "    farm = job['farm']\n",
        "    upload_token = job.get('upload_token', job_id)\n",
        "\n",
        "    print(f\"\\n‚úÖ Using backend: {api_base}\")\n",
        "\n",
        "# ‚ö†Ô∏è VALIDATE DATE - Must be in the past!\n",
        "try:\n",
        "    date_obj = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "    today = datetime.now()\n",
        "    days_ago = (today - date_obj).days\n",
        "\n",
        "    if days_ago < 0:\n",
        "        print(f\"\\n‚ùå ERROR: Date {date} is in the FUTURE!\")\n",
        "        print(f\"   Current date: {today.strftime('%Y-%m-%d')}\")\n",
        "        print(f\"   Sentinel-2 data only exists for past dates.\")\n",
        "        print(f\"\\nüí° FIXING: Using date from 30 days ago instead...\")\n",
        "\n",
        "        # Fix: Use 30 days ago\n",
        "        date = (today - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "        date_obj = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "        days_ago = 30\n",
        "\n",
        "        print(f\"   ‚úÖ Corrected date: {date}\")\n",
        "\n",
        "    elif days_ago < 5:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: Date {date} is only {days_ago} days ago\")\n",
        "        print(f\"   Sentinel-2 data has 2-5 day processing delay\")\n",
        "        print(f\"   If S2DR3 fails, try using date from 7+ days ago\")\n",
        "\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Date validation passed ({days_ago} days ago)\")\n",
        "\n",
        "except ValueError:\n",
        "    print(f\"\\n‚ö†Ô∏è  Invalid date format: {date}\")\n",
        "    print(f\"   Expected: YYYY-MM-DD\")\n",
        "\n",
        "print(f\"   üìç Location: ({lat:.4f}, {lon:.4f})\")\n",
        "print(f\"   üìÖ Date: {date}\")\n",
        "print(f\"   üîí Upload token: {upload_token[:8]}...\")\n",
        "print()"
      ],
      "metadata": {
        "id": "hqvQy2tnYuMs",
        "outputId": "90ec4c62-5ef4-446f-c54f-79e0f3255492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° Fetching job parameters...\n",
            "‚úÖ Job loaded from backend via tunnel!\n",
            "\n",
            "‚úÖ Using backend: https://slimy-points-tease.loca.lt\n",
            "   ‚úÖ Date validation passed (30 days ago)\n",
            "   üìç Location: (22.4245, 71.6444)\n",
            "   üìÖ Date: 2025-09-08\n",
            "   üîí Upload token: 0aa1687e...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üõ†Ô∏è Step 2: Install S2DR3 Wheel\n",
        "import subprocess\n",
        "\n",
        "wheel_url = \"https://storage.googleapis.com/0x7ff601307fa5/s2dr3-20250905.1-cp312-cp312-linux_x86_64.whl\"\n",
        "\n",
        "print(\"üì¶ Installing S2DR3 (Gamma Earth)...\")\n",
        "print(\"   This may take 1-2 minutes...\\n\")\n",
        "\n",
        "!pip install -q {wheel_url}\n",
        "\n",
        "print(\"\\n‚úÖ S2DR3 installed successfully!\")\n",
        "\n",
        "#@title üéØ Step 3: Run S2DR3 Super-Resolution\n",
        "import s2dr3.inferutils\n",
        "import pathlib\n",
        "\n",
        "print(\"üöÄ Running S2DR3 inference...\")\n",
        "print(f\"   üìç Target: ({lat:.4f}, {lon:.4f})\")\n",
        "print(f\"   üìÖ Date: {date}\")\n",
        "print(\"   ‚è±Ô∏è  This will take 2-4 minutes...\\n\")\n",
        "\n",
        "# Run S2DR3\n",
        "lonlat = (lon, lat)\n",
        "s2dr3.inferutils.test(lonlat, date)\n",
        "\n",
        "# Find output files\n",
        "output_dir = pathlib.Path(\"/content/output\")\n",
        "output_files = list(output_dir.glob(\"*.tif\"))\n",
        "\n",
        "print(f\"\\n‚úÖ S2DR3 complete! Generated {len(output_files)} files:\")\n",
        "for f in output_files:\n",
        "    size_mb = f.stat().st_size / 1024 / 1024\n",
        "    print(f\"   üìÑ {f.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "# Identify products\n",
        "ms_file = next((f for f in output_files if \"_MS.tif\" in f.name), None)\n",
        "ndvi_file = next((f for f in output_files if \"_NDVI.tif\" in f.name), None)\n",
        "tci_file = next((f for f in output_files if \"_TCI.tif\" in f.name), None)\n",
        "irp_file = next((f for f in output_files if \"_IRP.tif\" in f.name), None)\n",
        "\n",
        "if not ms_file or not ndvi_file:\n",
        "    raise Exception(\"‚ùå Missing required outputs (MS or NDVI)\")"
      ],
      "metadata": {
        "id": "q5a0v1mfZDn0",
        "outputId": "8b7214a1-3bd4-49cb-da4f-bd7e589de2f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing S2DR3 (Gamma Earth)...\n",
            "   This may take 1-2 minutes...\n",
            "\n",
            "\n",
            "‚úÖ S2DR3 installed successfully!\n",
            "üöÄ Running S2DR3 inference...\n",
            "   üìç Target: (22.4245, 71.6444)\n",
            "   üìÖ Date: 2025-09-08\n",
            "   ‚è±Ô∏è  This will take 2-4 minutes...\n",
            "\n",
            "Fetching data for T42QYK-d84bb1783-20250908 .......... done. \n",
            "Processing S2L2A_T42QYK-d84bb1783-20250908_MS "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚úÇÔ∏è Step 4: Clip to Farm Boundary (Optional)\n",
        "# Install rasterio for clipping\n",
        "!pip install -q rasterio\n",
        "\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "import json\n",
        "\n",
        "print(\"‚úÇÔ∏è  Clipping images to farm boundary...\\n\")\n",
        "\n",
        "def clip_to_farm(in_file, out_file, farm_geom):\n",
        "    \"\"\"Clip a GeoTIFF to farm boundary\"\"\"\n",
        "    try:\n",
        "        with rasterio.open(in_file) as src:\n",
        "            # Extract geometry\n",
        "            if farm_geom['type'] == 'Feature':\n",
        "                geom = farm_geom['geometry']\n",
        "            else:\n",
        "                geom = farm_geom\n",
        "\n",
        "            # Clip\n",
        "            out_image, out_transform = mask(src, [geom], crop=True, all_touched=True)\n",
        "            out_meta = src.meta.copy()\n",
        "\n",
        "            # Update metadata\n",
        "            out_meta.update({\n",
        "                \"driver\": \"GTiff\",\n",
        "                \"height\": out_image.shape[1],\n",
        "                \"width\": out_image.shape[2],\n",
        "                \"transform\": out_transform,\n",
        "                \"compress\": \"deflate\"\n",
        "            })\n",
        "\n",
        "            # Write\n",
        "            with rasterio.open(out_file, \"w\", **out_meta) as dest:\n",
        "                dest.write(out_image)\n",
        "                # Copy colormap if exists\n",
        "                if src.colormap(1):\n",
        "                    dest.write_colormap(1, src.colormap(1))\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Clipping failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create clipped directory\n",
        "clipped_dir = pathlib.Path(\"/content/clipped\")\n",
        "clipped_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Clip each file\n",
        "clipped_files = {}\n",
        "\n",
        "if ms_file:\n",
        "    out_ms = clipped_dir / \"ms_clipped.tif\"\n",
        "    if clip_to_farm(ms_file, out_ms, farm):\n",
        "        clipped_files['ms'] = out_ms\n",
        "        print(f\"   ‚úÖ MS clipped\")\n",
        "\n",
        "if ndvi_file:\n",
        "    out_ndvi = clipped_dir / \"ndvi_clipped.tif\"\n",
        "    if clip_to_farm(ndvi_file, out_ndvi, farm):\n",
        "        clipped_files['ndvi'] = out_ndvi\n",
        "        print(f\"   ‚úÖ NDVI clipped\")\n",
        "\n",
        "if tci_file:\n",
        "    out_tci = clipped_dir / \"tci_clipped.tif\"\n",
        "    if clip_to_farm(tci_file, out_tci, farm):\n",
        "        clipped_files['tci'] = out_tci\n",
        "        print(f\"   ‚úÖ TCI clipped\")\n",
        "\n",
        "print(f\"\\n‚úÖ Clipped {len(clipped_files)} images to farm boundary\")"
      ],
      "metadata": {
        "id": "68GIz8i-ZJnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì§ Step 5: Upload to DHARTI\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"üì§ Uploading enhanced images to DHARTI...\\n\")\n",
        "\n",
        "upload_url = f\"{api_base}/api/s2dr3/jobs/{job_id}/upload\"\n",
        "headers = {\"Authorization\": f\"Bearer {upload_token}\"}\n",
        "\n",
        "# Prepare files for upload\n",
        "files_to_upload = {}\n",
        "\n",
        "if 'ms' in clipped_files:\n",
        "    files_to_upload['ms'] = ('ms.tif', open(clipped_files['ms'], 'rb'), 'image/tiff')\n",
        "    print(f\"   üì¶ MS (multispectral) ready...\")\n",
        "\n",
        "if 'ndvi' in clipped_files:\n",
        "    files_to_upload['ndvi'] = ('ndvi.tif', open(clipped_files['ndvi'], 'rb'), 'image/tiff')\n",
        "    print(f\"   üì¶ NDVI ready...\")\n",
        "\n",
        "if 'tci' in clipped_files:\n",
        "    files_to_upload['tci'] = ('tci.tif', open(clipped_files['tci'], 'rb'), 'image/tiff')\n",
        "    print(f\"   üì¶ TCI (true color) ready...\")\n",
        "\n",
        "# Upload with progress bar\n",
        "print(f\"\\n‚è≥ Uploading to {upload_url}...\")\n",
        "\n",
        "try:\n",
        "    response = requests.post(upload_url, headers=headers, files=files_to_upload, timeout=300)\n",
        "\n",
        "    # Close file handles\n",
        "    for _, (_, f, _) in files_to_upload.items():\n",
        "        f.close()\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(f\"\\n‚úÖ Upload successful!\")\n",
        "        print(f\"   üìä Status: {result.get('status', 'unknown')}\")\n",
        "\n",
        "        saved_files = result.get('saved', {})\n",
        "        if saved_files:\n",
        "            print(f\"   üìÅ Saved files:\")\n",
        "            for key, path in saved_files.items():\n",
        "                print(f\"      ‚Ä¢ {key}: {path}\")\n",
        "\n",
        "        print(f\"\\nüéâ Your enhanced 1m images are now available in DHARTI!\")\n",
        "        print(f\"   üîÑ Refresh your browser to see the super-resolved layer.\")\n",
        "        print(f\"   üó∫Ô∏è  It will appear as: 'NDVI (S2DR3 1m, AI-enhanced)'\")\n",
        "\n",
        "    elif response.status_code == 403:\n",
        "        print(f\"\\n‚ùå Upload denied: Invalid authentication token\")\n",
        "        print(f\"   This job may have expired or the token is incorrect.\")\n",
        "\n",
        "    elif response.status_code == 404:\n",
        "        print(f\"\\n‚ùå Job not found: {job_id}\")\n",
        "        print(f\"   The job may have been deleted or doesn't exist.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Upload failed: HTTP {response.status_code}\")\n",
        "        print(f\"   Response: {response.text[:200]}\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"\\n‚è±Ô∏è  Upload timeout (>5 minutes)\")\n",
        "    print(f\"   Your files may still be uploading. Check DHARTI status.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Upload error: {e}\")\n",
        "    print(f\"\\nüí° You can manually download the files from /content/clipped/\")\n",
        "    print(f\"   Then contact support to upload them manually.\")"
      ],
      "metadata": {
        "id": "ElG-p4ygZRHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Pipeline Complete!\n",
        "\n",
        "The S2DR3 super-resolution has been successfully applied to your satellite imagery!\n",
        "\n",
        "### What was processed:\n",
        "\n",
        "- **Input**: Sentinel-2 10m resolution\n",
        "- **Output**: AI-enhanced 1m resolution (10x improvement!)\n",
        "- **Layers**: RGB (TCI), NDVI, Multi-spectral (10 bands)\n",
        "- **Processing Time**: ~3-5 minutes on GPU\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "1. **Return to DHARTI** web interface\n",
        "2. The modal will **automatically refresh** and show your enhanced images\n",
        "3. Use the **RGB/NDVI toggle** to switch between visualizations\n",
        "4. **Download** high-resolution GeoTIFFs if needed\n",
        "\n",
        "### Technical Details\n",
        "\n",
        "- **Model**: S2DR3 (Sentinel-2 Deep Residual Refine & Restore)\n",
        "- **Resolution**: 10x super-resolution (10m ‚Üí 1m)\n",
        "- **Architecture**: ResNet-based deep learning model\n",
        "- **Output Format**: GeoTIFF with proper georeferencing\n",
        "- **CRS**: EPSG:4326 (WGS84)\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "If you don't see the images in DHARTI:\n",
        "\n",
        "1. Check that the upload completed successfully (green ‚úÖ above)\n",
        "2. Refresh the DHARTI page\n",
        "3. Check browser console for errors\n",
        "4. Verify the job_id matches what you clicked in DHARTI\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Powered by DHARTI + S2DR3**\n",
        "\n",
        "*This automated pipeline brings professional-grade AI super-resolution to agricultural satellite imagery!*\n"
      ],
      "metadata": {
        "id": "UiHkfap8c9R1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7agNXmSpc6V8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}