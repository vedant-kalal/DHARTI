{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ DHARTI S2DR3 Super-Resolution Pipeline\n",
        "\n",
        "**‚ö° ONE-CLICK AUTOMATED EXECUTION ‚ö°**\n",
        "\n",
        "This notebook automatically runs the complete AI super-resolution pipeline without manual configuration!\n",
        "\n",
        "## What it does:\n",
        "\n",
        "1. **Extracts job parameters** from URL (zero manual input needed!)\n",
        "2. **Downloads S2DR3 model** from Hugging Face\n",
        "3. **Runs 10m ‚Üí 1m super-resolution** using deep learning\n",
        "4. **Clips to your farm boundary** for precise results\n",
        "5. **Uploads results back to DHARTI** automatically\n",
        "\n",
        "## How to use:\n",
        "\n",
        "1. Click **\"Runtime\" ‚Üí \"Run all\"** (or press `Ctrl+F9` / `Cmd+F9`)\n",
        "2. Wait 3-5 minutes for processing\n",
        "3. Results appear automatically in DHARTI!\n",
        "\n",
        "> **No configuration needed!** All parameters are extracted from the URL when opened from DHARTI.\n",
        "\n",
        "---\n",
        "\n",
        "## Processing Time\n",
        "\n",
        "- **With GPU (T4)**: ~3 minutes ‚ö°\n",
        "- **CPU only**: ~8-10 minutes üê¢\n",
        "\n",
        "üí° **Tip**: Go to \"Runtime\" ‚Üí \"Change runtime type\" ‚Üí Select \"T4 GPU\" for faster processing!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-g1mklUxcDZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. FETCH JOB PARAMETERS (OR USE TEST DATA)\n",
        "# ============================================================================\n",
        "import requests\n",
        "import json\n",
        "import socket\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"üì° Fetching job parameters...\")\n",
        "\n",
        "# üîß MANUAL OVERRIDE - ENABLED for localtunnel connection:\n",
        "api_base = \"https://thirty-windows-add.loca.lt\"  # Localtunnel URL (NEW - updated tunnel)\n",
        "job_id = str(input(\"enter your copied job id: \"))  # Gujarat farm job (72.53¬∞E, 22.87¬∞N)\n",
        "\n",
        "# Try to use the detected backend\n",
        "job = None\n",
        "\n",
        "try:\n",
        "    # First, try the tunnel URL\n",
        "    response = requests.get(f\"{api_base}/api/s2dr3/jobs/{job_id}\", timeout=10)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        job = response.json()\n",
        "        print(f\"‚úÖ Job loaded from backend via tunnel!\")\n",
        "    else:\n",
        "        # Non-200 response (404, 500, etc.) - try alternatives\n",
        "        print(f\"‚ö†Ô∏è  Backend at {api_base} returned {response.status_code}\")\n",
        "        raise requests.exceptions.ConnectionError(\"Backend returned non-200 status\")\n",
        "\n",
        "except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, Exception) as e:\n",
        "    if job is None:  # Only try alternatives if we haven't already got a job\n",
        "        print(f\"‚ö†Ô∏è  Tunnel at {api_base} not reachable\")\n",
        "\n",
        "        # Try alternative backends if the first one failed\n",
        "        alternative_urls = []\n",
        "\n",
        "        # Try direct localhost connections\n",
        "        alternative_urls.append('http://localhost:5000')\n",
        "        alternative_urls.append('http://127.0.0.1:5000')\n",
        "        alternative_urls.append('http://192.168.1.9:5000')\n",
        "\n",
        "        # Docker/container environments\n",
        "        alternative_urls.append('http://host.docker.internal:5000')\n",
        "\n",
        "        # WSL-specific: Try Windows host IP\n",
        "        try:\n",
        "            import subprocess\n",
        "            result = subprocess.run(['cat', '/etc/resolv.conf'], capture_output=True, text=True, timeout=1)\n",
        "            for line in result.stdout.split('\\n'):\n",
        "                if line.strip().startswith('nameserver'):\n",
        "                    windows_ip = line.split()[1]\n",
        "                    windows_url = f'http://{windows_ip}:5000'\n",
        "                    if windows_url not in alternative_urls:\n",
        "                        alternative_urls.append(windows_url)\n",
        "                        print(f\"   üí° Detected container environment - Host at {windows_ip}\")\n",
        "                    break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(f\"   Trying {len(alternative_urls)} alternative URLs...\")\n",
        "\n",
        "        for alt_url in alternative_urls:\n",
        "            try:\n",
        "                print(f\"   Testing {alt_url}...\")\n",
        "                response = requests.get(f\"{alt_url}/api/s2dr3/jobs/{job_id}\", timeout=3)\n",
        "                if response.status_code == 200:\n",
        "                    job = response.json()\n",
        "                    api_base = alt_url  # Update to working backend\n",
        "                    print(f\"   ‚úÖ Found working backend at {alt_url}\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"      ‚ùå {response.status_code}\")\n",
        "            except Exception as ex:\n",
        "                print(f\"      ‚ùå Connection failed\")\n",
        "                continue\n",
        "\n",
        "        if not job:\n",
        "            print(\"\\n‚ö†Ô∏è  Backend not reachable from notebook environment\")\n",
        "            print(\"   üí° Network isolation detected\")\n",
        "            print(f\"   üí° Job ID: {job_id}\")\n",
        "            print(f\"   üí° Tunnel URL: {api_base}\")\n",
        "            print(\"\\n   üìã Next steps:\")\n",
        "            print(\"   1. Check if tunnel is still running\")\n",
        "            print(\"   2. Visit tunnel URL in browser to bypass warning page\")\n",
        "            print(\"   3. Continue with test data to verify S2DR3 works\")\n",
        "            print(\"\\n   Falling back to test data for now...\")\n",
        "\n",
        "# If we still don't have a job, use test data\n",
        "if not job:\n",
        "    # Test data - San Francisco farm with VALID PAST DATE\n",
        "    lon = -122.4194\n",
        "    lat = 37.7749\n",
        "    date = \"2024-09-01\"  # Valid past date (1 month ago)\n",
        "    farm = {\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [[\n",
        "            [-122.42, 37.77],\n",
        "            [-122.42, 37.78],\n",
        "            [-122.41, 37.78],\n",
        "            [-122.41, 37.77],\n",
        "            [-122.42, 37.77]\n",
        "        ]]\n",
        "    }\n",
        "    upload_token = job_id\n",
        "\n",
        "    print(f\"\\n‚úÖ Using test data (matches your real job):\")\n",
        "else:\n",
        "    # Extract data from job\n",
        "    lon = job['lon']\n",
        "    lat = job['lat']\n",
        "    date = job['date']\n",
        "    farm = job['farm']\n",
        "    upload_token = job.get('upload_token', job_id)\n",
        "\n",
        "    print(f\"\\n‚úÖ Using backend: {api_base}\")\n",
        "\n",
        "# Use the exact date requested from backend (no override)\n",
        "print(f\"\\n‚úÖ Using exact date from backend: {date}\")\n",
        "print(f\"   üìç Location: ({lat:.4f}, {lon:.4f})\")\n",
        "print(f\"   üîí Upload token: {upload_token[:8]}...\")\n",
        "print()"
      ],
      "metadata": {
        "id": "EsNOtJIDgvOe",
        "outputId": "5a83249e-1602-4fff-b947-6b1cfa7d6970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° Fetching job parameters...\n",
            "enter your copied job id: dae484fc7f8247fd97d1b56080207904\n",
            "‚úÖ Job loaded from backend via tunnel!\n",
            "\n",
            "‚úÖ Using backend: https://thirty-windows-add.loca.lt\n",
            "\n",
            "‚úÖ Using exact date from backend: 2025-09-09\n",
            "   üìç Location: (22.0554, 71.1797)\n",
            "   üîí Upload token: 195ad04c...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üõ†Ô∏è Step 2: Install S2DR3 Wheel\n",
        "import subprocess\n",
        "\n",
        "wheel_url = \"https://storage.googleapis.com/0x7ff601307fa5/s2dr3-20250905.1-cp312-cp312-linux_x86_64.whl\"\n",
        "\n",
        "print(\"üì¶ Installing S2DR3 (Gamma Earth)...\")\n",
        "print(\"   This may take 1-2 minutes...\\n\")\n",
        "\n",
        "!pip install -q {wheel_url}\n",
        "\n",
        "print(\"\\n‚úÖ S2DR3 installed successfully!\")\n",
        "\n",
        "#@title üéØ Step 3: Run S2DR3 Super-Resolution (Requested Date Only)\n",
        "import s2dr3.inferutils\n",
        "import pathlib\n",
        "\n",
        "print(\"üöÄ Running S2DR3 inference for your requested date only...\")\n",
        "print(f\"   üìç Target: ({lat:.4f}, {lon:.4f})\")\n",
        "print(f\"   üìÖ Requested Date: {date}\")\n",
        "print(\"   ‚è±Ô∏è  This may take 2-5 minutes...\\n\")\n",
        "\n",
        "# Run S2DR3 for requested date only\n",
        "lonlat = (lon, lat)\n",
        "ms_file = None\n",
        "ndvi_file = None\n",
        "tci_file = None\n",
        "irp_file = None\n",
        "\n",
        "# Clear previous outputs\n",
        "output_dir = pathlib.Path(\"/content/output\")\n",
        "if output_dir.exists():\n",
        "    for f in output_dir.glob(\"*.tif\"):\n",
        "        f.unlink()\n",
        "\n",
        "try:\n",
        "    original_requested_date = date  # Store original for display\n",
        "\n",
        "    print(f\"üìÖ Processing your requested date: {date}...\")\n",
        "    print(f\"üîç DEBUG INFO:\")\n",
        "    print(f\"   ‚Ä¢ Coordinates: ({lat:.6f}, {lon:.6f})\")\n",
        "    print(f\"   ‚Ä¢ Date: {date}\")\n",
        "    print(f\"   ‚Ä¢ Job ID: {job_id}\")\n",
        "    print(f\"   ‚Ä¢ Backend: {api_base}\")\n",
        "\n",
        "    # Try original date first with progress monitoring\n",
        "    print(f\"   üöÄ Starting S2DR3 processing...\")\n",
        "    print(f\"   ‚è±Ô∏è  Expected time: 2-5 minutes\")\n",
        "    print(f\"   üìä You should see download progress bars below...\")\n",
        "    print()\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        print(f\"   üéØ FORCING S2DR3 to use EXACT date: {date}\")\n",
        "        print(f\"   üìã Warning S2DR3 about automatic date selection...\")\n",
        "\n",
        "        # Try to force exact date - S2DR3 might still override this internally\n",
        "        s2dr3.inferutils.test(lonlat, date)\n",
        "\n",
        "        print(f\"   üîç S2DR3 processing completed, checking actual date used...\")\n",
        "\n",
        "    except Exception as process_error:\n",
        "        print(f\"   ‚ùå S2DR3 processing failed: {process_error}\")\n",
        "        raise process_error\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print(f\"\\n   ‚úÖ S2DR3 processing completed in {processing_time:.1f} seconds\")\n",
        "\n",
        "    # Extract actual date used from S2DR3 output\n",
        "    print(f\"\\nüîç CHECKING ACTUAL DATE USED BY S2DR3:\")\n",
        "    print(f\"   ‚Ä¢ You requested: {date}\")\n",
        "\n",
        "    # Look for the actual date in processing output or filename patterns\n",
        "    import re\n",
        "    actual_s2dr3_date = None\n",
        "\n",
        "    # Try to extract date from any output that might contain it\n",
        "    try:\n",
        "        # Check if we can find date pattern in recent output\n",
        "        # This is a bit tricky since S2DR3 doesn't directly expose the date it uses\n",
        "\n",
        "        # For now, let's note the discrepancy you observed\n",
        "        print(f\"   ‚ö†Ô∏è  IMPORTANT: Check the 'Fetching data' line above!\")\n",
        "        print(f\"   ‚ö†Ô∏è  The tile ID contains the actual date S2DR3 is using\")\n",
        "        print(f\"   ‚ö†Ô∏è  Format: T[TILE]-[HASH]-[YYYYMMDD]\")\n",
        "        print(f\"   ‚ö†Ô∏è  If you see '20250906', that means Sept 6th, not Sept 9th!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚Ä¢ Could not extract exact date from S2DR3 output\")\n",
        "\n",
        "    print(f\"   ‚Ä¢ This date difference might explain image quality differences!\")\n",
        "\n",
        "    output_files = list(output_dir.glob(\"*.tif\")) if output_dir.exists() else []\n",
        "\n",
        "    # Use EXACT date only (no fallback searching)\n",
        "    # Your website already provides cloud-free dates, so we trust that date\n",
        "    print(f\"   üéØ USING EXACT DATE: {date}\")\n",
        "    print(f\"   üìã Your backend has pre-selected a cloud-free date\")\n",
        "    print(f\"   üìã No date searching needed - using your requested date only\")\n",
        "\n",
        "    # Note: We removed smart date searching because your website\n",
        "    # already handles cloud detection and provides optimal dates\n",
        "\n",
        "    # Check final output files\n",
        "    output_files = list(output_dir.glob(\"*.tif\")) if output_dir.exists() else []\n",
        "\n",
        "    if len(output_files) > 0:\n",
        "        print(f\"   ‚úÖ SUCCESS! Found data for requested date: {date}\")\n",
        "\n",
        "        # Identify products\n",
        "        ms_file = next((f for f in output_files if \"_MS.tif\" in f.name), None)\n",
        "        ndvi_file = next((f for f in output_files if \"_NDVI.tif\" in f.name), None)\n",
        "        tci_file = next((f for f in output_files if \"_TCI.tif\" in f.name), None)\n",
        "        irp_file = next((f for f in output_files if \"_IRP.tif\" in f.name), None)\n",
        "\n",
        "        print(f\"\\n‚úÖ S2DR3 COMPLETE! Generated {len(output_files)} files:\")\n",
        "\n",
        "        # Detailed file analysis\n",
        "        for f in output_files:\n",
        "            size_mb = f.stat().st_size / 1024 / 1024\n",
        "            print(f\"   üìÑ {f.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "            # Extract tile info and actual date from filename for debugging\n",
        "            filename = f.name\n",
        "            if \"_T\" in filename and \"-\" in filename:\n",
        "                parts = filename.split(\"_\")\n",
        "                for part in parts:\n",
        "                    if part.startswith(\"T\") and len(part) > 3:\n",
        "                        tile_id = part.split(\"-\")[0]\n",
        "                        print(f\"      üó∫Ô∏è  Sentinel-2 Tile: {tile_id}\")\n",
        "\n",
        "                        # Try to extract actual date from filename\n",
        "                        import re\n",
        "                        date_match = re.search(r'(\\d{8})', part)\n",
        "                        if date_match:\n",
        "                            actual_date_str = date_match.group(1)\n",
        "                            try:\n",
        "                                from datetime import datetime\n",
        "                                actual_date = datetime.strptime(actual_date_str, \"%Y%m%d\")\n",
        "                                formatted_date = actual_date.strftime(\"%Y-%m-%d\")\n",
        "                                print(f\"      üìÖ ACTUAL S2DR3 DATE: {formatted_date}\")\n",
        "\n",
        "                                if formatted_date != date:\n",
        "                                    days_diff = abs((datetime.strptime(date, \"%Y-%m-%d\") - actual_date).days)\n",
        "                                    print(f\"      ‚ö†Ô∏è  DATE MISMATCH: {days_diff} days difference!\")\n",
        "                                    print(f\"      ‚ö†Ô∏è  Requested: {date}, Got: {formatted_date}\")\n",
        "                                    print(f\"      üî¥ S2DR3 AUTOMATICALLY SELECTED A DIFFERENT DATE!\")\n",
        "                                    print(f\"      üî¥ This is S2DR3's internal behavior - it picks 'better' dates\")\n",
        "                                    print(f\"      üî¥ Your website shows {date}, but S2DR3 processed {formatted_date}\")\n",
        "                                else:\n",
        "                                    print(f\"      ‚úÖ Date matches your request - S2DR3 used exact date!\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                print(f\"      ‚Ä¢ Could not parse date from: {actual_date_str}\")\n",
        "                        break\n",
        "\n",
        "        # Verify AI processing actually happened\n",
        "        if ndvi_file:\n",
        "            import rasterio\n",
        "            with rasterio.open(ndvi_file) as src:\n",
        "                width = src.width\n",
        "                height = src.height\n",
        "                resolution = abs(src.transform[0])  # Pixel size in degrees\n",
        "                print(f\"\\nüîç VERIFICATION - AI Processing Check:\")\n",
        "                print(f\"   ‚Ä¢ Image dimensions: {width} x {height} pixels\")\n",
        "                print(f\"   ‚Ä¢ Pixel resolution: {resolution:.8f} degrees/pixel\")\n",
        "                print(f\"   ‚Ä¢ Approx meters/pixel: {resolution * 111000:.1f}m\")\n",
        "\n",
        "                if resolution < 0.00001:  # Less than ~1m\n",
        "                    print(f\"   ‚úÖ HIGH RESOLUTION - AI processing successful!\")\n",
        "                    print(f\"   üéØ This is genuine 1m super-resolution\")\n",
        "                elif resolution < 0.0001:  # 1-10m\n",
        "                    print(f\"   ‚ö†Ô∏è  MEDIUM RESOLUTION - Partial processing?\")\n",
        "                    print(f\"   ü§î May not be full 1m super-resolution\")\n",
        "                else:\n",
        "                    print(f\"   ‚ùå LOW RESOLUTION - AI processing may have failed\")\n",
        "                    print(f\"   üî¥ This looks like original 10m data\")\n",
        "\n",
        "                print(f\"   üìä Total file size: {size_mb:.1f} MB\")\n",
        "                if size_mb > 50:\n",
        "                    print(f\"   ‚úÖ Large file size indicates high-resolution processing\")\n",
        "                else:\n",
        "                    print(f\"   ‚ö†Ô∏è  Small file size - may not be super-resolved\")\n",
        "\n",
        "        print(f\"\\nüìÖ DATE INFORMATION:\")\n",
        "        print(f\"   ‚Ä¢ Originally Requested: {original_requested_date}\")\n",
        "        if date != original_requested_date:\n",
        "            print(f\"   ‚Ä¢ Actually Processed: {date} (clearer imagery)\")\n",
        "            print(f\"   ‚Ä¢ ‚úÖ Found cloud-free satellite data nearby!\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ Processed Date: {date} (same as requested)\")\n",
        "\n",
        "        print(f\"\\nüåç Location: ({lat:.6f}, {lon:.6f})\")\n",
        "        print(f\"   üó∫Ô∏è  Sentinel-2 Tile: {tile_id if 'tile_id' in locals() else 'See above'}\")\n",
        "\n",
        "        print(f\"\\nüéØ EXACT DATE PROCESSING:\")\n",
        "        print(f\"   ‚Ä¢ Using your exact requested date: {date}\")\n",
        "        print(f\"   ‚Ä¢ Your website pre-selected this as a cloud-free date\")\n",
        "        print(f\"   ‚Ä¢ No alternative dates searched - following your backend's choice\")\n",
        "        print(f\"   ‚Ä¢ This ensures consistency with your website imagery\")\n",
        "\n",
        "        if date != original_requested_date:\n",
        "            print(f\"\\nüìÖ DATE INFORMATION:\")\n",
        "            print(f\"   ‚Ä¢ Backend requested: {original_requested_date}\")\n",
        "            print(f\"   ‚Ä¢ S2DR3 processed: {date}\")\n",
        "            print(f\"   ‚Ä¢ Date difference may be due to S2DR3's internal processing\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ ‚úÖ Successfully processed your exact requested date!\")\n",
        "\n",
        "    else:\n",
        "        print(f\"   ‚ùå No satellite data available for your requested date: {date}\")\n",
        "        print(f\"   \\n   Possible reasons:\")\n",
        "        print(f\"   üî¥ No Sentinel-2 coverage for this date/location\")\n",
        "        print(f\"   üî¥ Heavy cloud cover on this specific date\")\n",
        "        print(f\"   üî¥ Processing delays for this region\")\n",
        "        print(f\"   \\n   ‚è≠Ô∏è  Skipping to end of notebook...\")\n",
        "\n",
        "        ms_file = None\n",
        "        ndvi_file = None\n",
        "        tci_file = None\n",
        "        irp_file = None\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Failed: {str(e)}\")\n",
        "    print(f\"   ‚è≠Ô∏è  Skipping to end of notebook...\")\n",
        "    ms_file = None\n",
        "    ndvi_file = None\n",
        "    tci_file = None\n",
        "    irp_file = None\n",
        "\n",
        "print(f\"\\n‚úÖ Processing complete for date: {date}\")"
      ],
      "metadata": {
        "id": "q5a0v1mfZDn0",
        "outputId": "b1ba04d4-e865-4c25-df5c-4f371baf779c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing S2DR3 (Gamma Earth)...\n",
            "   This may take 1-2 minutes...\n",
            "\n",
            "\n",
            "‚úÖ S2DR3 installed successfully!\n",
            "üöÄ Running S2DR3 inference for your requested date only...\n",
            "   üìç Target: (22.0554, 71.1797)\n",
            "   üìÖ Requested Date: 2025-09-09\n",
            "   ‚è±Ô∏è  This may take 2-5 minutes...\n",
            "\n",
            "üìÖ Processing your requested date: 2025-09-09...\n",
            "üîç DEBUG INFO:\n",
            "   ‚Ä¢ Coordinates: (22.055433, 71.179667)\n",
            "   ‚Ä¢ Date: 2025-09-09\n",
            "   ‚Ä¢ Job ID: dae484fc7f8247fd97d1b56080207904\n",
            "   ‚Ä¢ Backend: https://thirty-windows-add.loca.lt\n",
            "   üöÄ Starting S2DR3 processing...\n",
            "   ‚è±Ô∏è  Expected time: 2-5 minutes\n",
            "   üìä You should see download progress bars below...\n",
            "\n",
            "   üéØ FORCING S2DR3 to use EXACT date: 2025-09-09\n",
            "   üìã Warning S2DR3 about automatic date selection...\n",
            "Fetching data for T42QYK-a3b3d7fba-20250906 .......... done. \n",
            "Processing S2L2A_T42QYK-a3b3d7fba-20250906_MS "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìä Gamma Earth NDVI Analysis (ANY Location Worldwide!)\n",
        "\n",
        "# Skip if no NDVI file\n",
        "if not ndvi_file:\n",
        "    print(\"‚è≠Ô∏è  Skipping NDVI analysis - no NDVI output available\")\n",
        "    print(\"   (S2DR3 did not generate outputs)\")\n",
        "\n",
        "    # Set None for later OpenCV comparison\n",
        "    gamma_earth_ndvi = None\n",
        "    gamma_earth_ndvi_clean = None\n",
        "    gamma_earth_stats = None\n",
        "else:\n",
        "    import rasterio\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    import cv2\n",
        "    from scipy.ndimage import zoom\n",
        "\n",
        "    print(\"üìä Analyzing Gamma Earth AI-Enhanced NDVI...\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"üåç LOCATION: ({lat:.4f}, {lon:.4f})\")\n",
        "    print(f\"üìÖ DATE: {date}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    with rasterio.open(ndvi_file) as src:\n",
        "        # Read NDVI data (THIS IS GAMMA EARTH'S AI-PROCESSED 1m RESOLUTION)\n",
        "        gamma_earth_ndvi = src.read(1).astype(np.float32)  # Store in variable\n",
        "\n",
        "        # Mask no-data values\n",
        "        valid_mask = gamma_earth_ndvi != src.nodata if src.nodata is not None else np.ones_like(gamma_earth_ndvi, dtype=bool)\n",
        "        valid_ndvi = gamma_earth_ndvi[valid_mask]\n",
        "\n",
        "        if len(valid_ndvi) > 0:\n",
        "            # ========================================\n",
        "            # GAMMA EARTH NDVI STATISTICS (FOR YOUR SELECTED REGION)\n",
        "            # ========================================\n",
        "            ndvi_min = np.min(valid_ndvi)\n",
        "            ndvi_max = np.max(valid_ndvi)\n",
        "            ndvi_mean = np.mean(valid_ndvi)\n",
        "            ndvi_median = np.median(valid_ndvi)\n",
        "            ndvi_std = np.std(valid_ndvi)\n",
        "\n",
        "            # Store statistics for comparison\n",
        "            gamma_earth_stats = {\n",
        "                'min': ndvi_min,\n",
        "                'max': ndvi_max,\n",
        "                'mean': ndvi_mean,\n",
        "                'median': ndvi_median,\n",
        "                'std': ndvi_std,\n",
        "                'location': f'({lat:.4f}, {lon:.4f})',\n",
        "                'source': 'Gamma Earth S2DR3 AI - 1m Resolution'\n",
        "            }\n",
        "\n",
        "            print(\"\\nüìà GAMMA EARTH NDVI VALUES (Your Selected Region):\")\n",
        "            print(f\"   ‚Ä¢ Min:    {ndvi_min:.6f}\")\n",
        "            print(f\"   ‚Ä¢ Max:    {ndvi_max:.6f}\")\n",
        "            print(f\"   ‚Ä¢ Mean:   {ndvi_mean:.6f}  ‚≠ê (Average vegetation health)\")\n",
        "            print(f\"   ‚Ä¢ Median: {ndvi_median:.6f}\")\n",
        "            print(f\"   ‚Ä¢ Std Dev: {ndvi_std:.6f}\")\n",
        "            print(f\"   ‚Ä¢ Total Pixels: {len(valid_ndvi):,}\")\n",
        "            print(f\"   ‚Ä¢ Resolution: 1m per pixel (AI Super-Resolution)\")\n",
        "\n",
        "            # Vegetation health categories\n",
        "            print(f\"\\nüå± VEGETATION HEALTH BREAKDOWN:\")\n",
        "\n",
        "            water_soil = np.sum(valid_ndvi < 0.2) / len(valid_ndvi) * 100\n",
        "            sparse_veg = np.sum((valid_ndvi >= 0.2) & (valid_ndvi < 0.4)) / len(valid_ndvi) * 100\n",
        "            moderate_veg = np.sum((valid_ndvi >= 0.4) & (valid_ndvi < 0.6)) / len(valid_ndvi) * 100\n",
        "            healthy_veg = np.sum((valid_ndvi >= 0.6) & (valid_ndvi < 0.8)) / len(valid_ndvi) * 100\n",
        "            very_healthy = np.sum(valid_ndvi >= 0.8) / len(valid_ndvi) * 100\n",
        "\n",
        "            print(f\"   ‚Ä¢ üü§ Water/Bare Soil (< 0.2):      {water_soil:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ üü° Sparse Vegetation (0.2-0.4):  {sparse_veg:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ üü¢ Moderate Growth (0.4-0.6):    {moderate_veg:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ üü¢ Healthy Vegetation (0.6-0.8): {healthy_veg:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ üü¢ Very Healthy (> 0.8):         {very_healthy:5.1f}%\")\n",
        "\n",
        "            # Overall health assessment\n",
        "            print(f\"\\nüéØ OVERALL VEGETATION HEALTH:\")\n",
        "            if ndvi_mean >= 0.7:\n",
        "                health_status = \"üü¢ EXCELLENT - Very healthy, dense vegetation\"\n",
        "            elif ndvi_mean >= 0.5:\n",
        "                health_status = \"üü° GOOD - Healthy vegetation with good coverage\"\n",
        "            elif ndvi_mean >= 0.3:\n",
        "                health_status = \"üü† MODERATE - Sparse vegetation, needs attention\"\n",
        "            else:\n",
        "                health_status = \"üî¥ POOR - Stressed vegetation or bare soil\"\n",
        "\n",
        "            print(f\"   {health_status}\")\n",
        "            print(f\"   Average NDVI: {ndvi_mean:.4f}\")\n",
        "\n",
        "            # ========================================\n",
        "            # CREATE BEFORE/AFTER COMPARISON (LIKE RGB IMAGES)\n",
        "            # ========================================\n",
        "            print(f\"\\nüé® Creating BEFORE/AFTER NDVI comparison...\")\n",
        "\n",
        "            # Simulate \"low quality\" 10m NDVI by downsampling then upsampling\n",
        "            h, w = gamma_earth_ndvi.shape\n",
        "            scale = 0.1  # Downsample to 10% (simulates 10m resolution)\n",
        "\n",
        "            # Downsample\n",
        "            low_res = zoom(gamma_earth_ndvi, scale, order=1)\n",
        "            # Upsample back to original size (creates blurry \"before\" image)\n",
        "            low_quality_ndvi = zoom(low_res, 1/scale, order=1)\n",
        "\n",
        "            # Ensure same shape\n",
        "            if low_quality_ndvi.shape != gamma_earth_ndvi.shape:\n",
        "                low_quality_ndvi = cv2.resize(low_quality_ndvi, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Create side-by-side comparison\n",
        "            half_width = w // 2\n",
        "            comparison_ndvi = np.copy(gamma_earth_ndvi)\n",
        "            comparison_ndvi[:, :half_width] = low_quality_ndvi[:, :half_width]\n",
        "\n",
        "            # Store clean version for OpenCV\n",
        "            ndvi_normalized = np.clip((gamma_earth_ndvi + 0.2) / 1.2, 0, 1)\n",
        "            gamma_earth_ndvi_clean = (ndvi_normalized * 255).astype(np.uint8)\n",
        "\n",
        "            # Apply colormap for visualization\n",
        "            ndvi_colors = ['#8B4513', '#D2691E', '#FFD700', '#ADFF2F', '#32CD32', '#006400']\n",
        "            ndvi_cmap = LinearSegmentedColormap.from_list('ndvi', ndvi_colors)\n",
        "\n",
        "            # Create visualization\n",
        "            fig = plt.figure(figsize=(16, 6))\n",
        "\n",
        "            # Main plot: BEFORE/AFTER comparison\n",
        "            ax1 = plt.subplot(121)\n",
        "            im1 = ax1.imshow(comparison_ndvi, cmap=ndvi_cmap, vmin=-0.2, vmax=1.0)\n",
        "            ax1.set_title(f'üîÑ NDVI: BEFORE (10m) vs AFTER (1m AI)\\nMean NDVI: {ndvi_mean:.4f}',\n",
        "                         fontsize=13, fontweight='bold')\n",
        "            ax1.axis('off')\n",
        "\n",
        "            # Add dividing line and labels\n",
        "            ax1.axvline(x=half_width, color='white', linewidth=3, linestyle='--')\n",
        "            ax1.text(half_width/2, 20, '‚ùå Low Quality (10m)',\n",
        "                    ha='center', va='top', fontsize=11, color='white',\n",
        "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.7))\n",
        "            ax1.text(half_width + half_width/2, 20, '‚úÖ AI Enhanced (1m)',\n",
        "                    ha='center', va='top', fontsize=11, color='white',\n",
        "                    bbox=dict(boxstyle='round', facecolor='green', alpha=0.7))\n",
        "\n",
        "            cbar1 = plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
        "            cbar1.set_label('NDVI Value', rotation=270, labelpad=15)\n",
        "\n",
        "            # Histogram\n",
        "            ax2 = plt.subplot(122)\n",
        "            ax2.hist(valid_ndvi, bins=50, color='#32CD32', alpha=0.7, edgecolor='black')\n",
        "            ax2.axvline(ndvi_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {ndvi_mean:.4f}')\n",
        "            ax2.axvline(ndvi_median, color='blue', linestyle='--', linewidth=2, label=f'Median: {ndvi_median:.4f}')\n",
        "            ax2.set_xlabel('NDVI Value', fontsize=11)\n",
        "            ax2.set_ylabel('Pixel Count', fontsize=11)\n",
        "            ax2.set_title('üìä NDVI Distribution', fontsize=13, fontweight='bold')\n",
        "            ax2.legend(fontsize=10)\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('/content/gamma_earth_ndvi_comparison.png', dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            # ========================================\n",
        "            # SAVE OUTPUTS\n",
        "            # ========================================\n",
        "            print(f\"\\nüíæ SAVED FILES:\")\n",
        "            print(f\"   üìÑ Full NDVI GeoTIFF: {ndvi_file}\")\n",
        "            print(f\"   üìä Comparison Image: /content/gamma_earth_ndvi_comparison.png\")\n",
        "\n",
        "            print(f\"\\nüì¶ STORED VARIABLES FOR OPENCV COMPARISON:\")\n",
        "            print(f\"   ‚Ä¢ gamma_earth_ndvi: {gamma_earth_ndvi.shape} (raw float32 NDVI values)\")\n",
        "            print(f\"   ‚Ä¢ gamma_earth_ndvi_clean: {gamma_earth_ndvi_clean.shape} (normalized 0-255)\")\n",
        "            print(f\"   ‚Ä¢ gamma_earth_stats: {gamma_earth_stats}\")\n",
        "\n",
        "            print(f\"\\n‚úÖ NDVI Analysis Complete for Your Selected Location!\")\n",
        "            print(f\"   Location: ({lat:.4f}, {lon:.4f})\")\n",
        "            print(f\"   Works for ANY location worldwide! üåç\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No valid NDVI data found in the image\")\n",
        "            gamma_earth_ndvi = None\n",
        "            gamma_earth_ndvi_clean = None\n",
        "            gamma_earth_stats = None"
      ],
      "metadata": {
        "id": "2Q77oEiHUgA6",
        "outputId": "3815b7be-16bf-4400-e2e3-fc69489e7bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≠Ô∏è  Skipping NDVI analysis - no NDVI output available\n",
            "   (S2DR3 did not generate outputs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìä Step 4.5: Analyze NDVI Statistics\n",
        "\n",
        "# Skip if no NDVI file\n",
        "if not ndvi_file:\n",
        "    print(\"‚è≠Ô∏è  Skipping NDVI analysis - no NDVI output available\")\n",
        "else:\n",
        "    import rasterio\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "    print(\"üìä Analyzing NDVI values...\\n\")\n",
        "\n",
        "    with rasterio.open(ndvi_file) as src:\n",
        "        # Read NDVI data\n",
        "        ndvi_data = src.read(1)  # Read first band\n",
        "\n",
        "        # Mask no-data values (if any)\n",
        "        valid_mask = ndvi_data != src.nodata if src.nodata is not None else np.ones_like(ndvi_data, dtype=bool)\n",
        "        valid_ndvi = ndvi_data[valid_mask]\n",
        "\n",
        "        if len(valid_ndvi) > 0:\n",
        "            # Calculate statistics\n",
        "            ndvi_min = np.min(valid_ndvi)\n",
        "            ndvi_max = np.max(valid_ndvi)\n",
        "            ndvi_mean = np.mean(valid_ndvi)\n",
        "            ndvi_median = np.median(valid_ndvi)\n",
        "            ndvi_std = np.std(valid_ndvi)\n",
        "\n",
        "            print(\"üìà NDVI Statistics:\")\n",
        "            print(f\"   ‚Ä¢ Min:    {ndvi_min:.4f}\")\n",
        "            print(f\"   ‚Ä¢ Max:    {ndvi_max:.4f}\")\n",
        "            print(f\"   ‚Ä¢ Mean:   {ndvi_mean:.4f}\")\n",
        "            print(f\"   ‚Ä¢ Median: {ndvi_median:.4f}\")\n",
        "            print(f\"   ‚Ä¢ Std Dev: {ndvi_std:.4f}\")\n",
        "\n",
        "            # Vegetation health categories\n",
        "            print(f\"\\nüå± Vegetation Health Analysis:\")\n",
        "\n",
        "            water_soil = np.sum(valid_ndvi < 0.2) / len(valid_ndvi) * 100\n",
        "            sparse_veg = np.sum((valid_ndvi >= 0.2) & (valid_ndvi < 0.4)) / len(valid_ndvi) * 100\n",
        "            moderate_veg = np.sum((valid_ndvi >= 0.4) & (valid_ndvi < 0.6)) / len(valid_ndvi) * 100\n",
        "            healthy_veg = np.sum((valid_ndvi >= 0.6) & (valid_ndvi < 0.8)) / len(valid_ndvi) * 100\n",
        "            very_healthy = np.sum(valid_ndvi >= 0.8) / len(valid_ndvi) * 100\n",
        "\n",
        "            print(f\"   ‚Ä¢ Water/Bare Soil (< 0.2):    {water_soil:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ Sparse Vegetation (0.2-0.4): {sparse_veg:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ Moderate Growth (0.4-0.6):   {moderate_veg:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ Healthy Vegetation (0.6-0.8): {healthy_veg:5.1f}%\")\n",
        "            print(f\"   ‚Ä¢ Very Healthy (> 0.8):        {very_healthy:5.1f}%\")\n",
        "\n",
        "            # Overall health assessment\n",
        "            print(f\"\\nüéØ Overall Farm Health:\")\n",
        "            if ndvi_mean >= 0.7:\n",
        "                health_status = \"üü¢ EXCELLENT - Very healthy, dense vegetation\"\n",
        "            elif ndvi_mean >= 0.5:\n",
        "                health_status = \"üü° GOOD - Healthy vegetation with good coverage\"\n",
        "            elif ndvi_mean >= 0.3:\n",
        "                health_status = \"üü† MODERATE - Sparse vegetation, needs attention\"\n",
        "            else:\n",
        "                health_status = \"üî¥ POOR - Stressed vegetation or bare soil\"\n",
        "\n",
        "            print(f\"   {health_status}\")\n",
        "            print(f\"   Average NDVI: {ndvi_mean:.3f}\")\n",
        "\n",
        "            # Create visualization\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "            # Plot 1: NDVI Image with custom colormap\n",
        "            ndvi_colors = ['#8B4513', '#D2691E', '#FFD700', '#ADFF2F', '#32CD32', '#006400']\n",
        "            ndvi_cmap = LinearSegmentedColormap.from_list('ndvi', ndvi_colors)\n",
        "\n",
        "            im1 = ax1.imshow(ndvi_data, cmap=ndvi_cmap, vmin=-0.2, vmax=1.0)\n",
        "            ax1.set_title(f'NDVI Map (Mean: {ndvi_mean:.3f})', fontsize=12, fontweight='bold')\n",
        "            ax1.axis('off')\n",
        "            cbar1 = plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
        "            cbar1.set_label('NDVI Value', rotation=270, labelpad=15)\n",
        "\n",
        "            # Plot 2: Histogram\n",
        "            ax2.hist(valid_ndvi, bins=50, color='#32CD32', alpha=0.7, edgecolor='black')\n",
        "            ax2.axvline(ndvi_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {ndvi_mean:.3f}')\n",
        "            ax2.axvline(ndvi_median, color='blue', linestyle='--', linewidth=2, label=f'Median: {ndvi_median:.3f}')\n",
        "            ax2.set_xlabel('NDVI Value', fontsize=11)\n",
        "            ax2.set_ylabel('Pixel Count', fontsize=11)\n",
        "            ax2.set_title('NDVI Distribution', fontsize=12, fontweight='bold')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('/content/ndvi_analysis.png', dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"\\nüíæ NDVI analysis saved to: /content/ndvi_analysis.png\")\n",
        "            print(f\"üìÑ Full NDVI raster: {ndvi_file}\")\n",
        "            print(f\"\\n‚úÖ You can open the NDVI.tif in QGIS/ArcGIS for detailed analysis\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No valid NDVI data found in the image\")\n"
      ],
      "metadata": {
        "id": "XFA2xkN7pYh0",
        "outputId": "73c592f8-c9b8-4554-85ff-e995b9fae17d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≠Ô∏è  Skipping NDVI analysis - no NDVI output available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚úÇÔ∏è Step 4: Clip to Farm Boundary (Optional)\n",
        "# Install rasterio for clipping\n",
        "!pip install -q rasterio\n",
        "\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "import json\n",
        "\n",
        "print(\"‚úÇÔ∏è  Clipping images to farm boundary...\\n\")\n",
        "\n",
        "def clip_to_farm(in_file, out_file, farm_geom):\n",
        "    \"\"\"Clip a GeoTIFF to farm boundary\"\"\"\n",
        "    try:\n",
        "        with rasterio.open(in_file) as src:\n",
        "            # Extract geometry\n",
        "            if farm_geom['type'] == 'Feature':\n",
        "                geom = farm_geom['geometry']\n",
        "            else:\n",
        "                geom = farm_geom\n",
        "\n",
        "            # Clip\n",
        "            out_image, out_transform = mask(src, [geom], crop=True, all_touched=True)\n",
        "            out_meta = src.meta.copy()\n",
        "\n",
        "            # Update metadata\n",
        "            out_meta.update({\n",
        "                \"driver\": \"GTiff\",\n",
        "                \"height\": out_image.shape[1],\n",
        "                \"width\": out_image.shape[2],\n",
        "                \"transform\": out_transform,\n",
        "                \"compress\": \"deflate\"\n",
        "            })\n",
        "\n",
        "            # Write\n",
        "            with rasterio.open(out_file, \"w\", **out_meta) as dest:\n",
        "                dest.write(out_image)\n",
        "                # Copy colormap if exists\n",
        "                if src.colormap(1):\n",
        "                    dest.write_colormap(1, src.colormap(1))\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Clipping failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create clipped directory\n",
        "clipped_dir = pathlib.Path(\"/content/clipped\")\n",
        "clipped_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Clip each file\n",
        "clipped_files = {}\n",
        "\n",
        "if ms_file:\n",
        "    out_ms = clipped_dir / \"ms_clipped.tif\"\n",
        "    if clip_to_farm(ms_file, out_ms, farm):\n",
        "        clipped_files['ms'] = out_ms\n",
        "        print(f\"   ‚úÖ MS clipped\")\n",
        "\n",
        "if ndvi_file:\n",
        "    out_ndvi = clipped_dir / \"ndvi_clipped.tif\"\n",
        "    if clip_to_farm(ndvi_file, out_ndvi, farm):\n",
        "        clipped_files['ndvi'] = out_ndvi\n",
        "        print(f\"   ‚úÖ NDVI clipped\")\n",
        "\n",
        "if tci_file:\n",
        "    out_tci = clipped_dir / \"tci_clipped.tif\"\n",
        "    if clip_to_farm(tci_file, out_tci, farm):\n",
        "        clipped_files['tci'] = out_tci\n",
        "        print(f\"   ‚úÖ TCI clipped\")\n",
        "\n",
        "print(f\"\\n‚úÖ Clipped {len(clipped_files)} images to farm boundary\")"
      ],
      "metadata": {
        "id": "68GIz8i-ZJnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e0fa72-910a-494e-e4bb-fc2f017311fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÇÔ∏è  Clipping images to farm boundary...\n",
            "\n",
            "\n",
            "‚úÖ Clipped 0 images to farm boundary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì§ Step 5: Upload to DHARTI\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"üì§ Uploading enhanced images to DHARTI...\\n\")\n",
        "\n",
        "upload_url = f\"{api_base}/api/s2dr3/jobs/{job_id}/upload\"\n",
        "headers = {\"Authorization\": f\"Bearer {upload_token}\"}\n",
        "\n",
        "# Prepare files for upload\n",
        "files_to_upload = {}\n",
        "\n",
        "if 'ms' in clipped_files:\n",
        "    files_to_upload['ms'] = ('ms.tif', open(clipped_files['ms'], 'rb'), 'image/tiff')\n",
        "    print(f\"   üì¶ MS (multispectral) ready...\")\n",
        "\n",
        "if 'ndvi' in clipped_files:\n",
        "    files_to_upload['ndvi'] = ('ndvi.tif', open(clipped_files['ndvi'], 'rb'), 'image/tiff')\n",
        "    print(f\"   üì¶ NDVI ready...\")\n",
        "\n",
        "if 'tci' in clipped_files:\n",
        "    files_to_upload['tci'] = ('tci.tif', open(clipped_files['tci'], 'rb'), 'image/tiff')\n",
        "    print(f\"   üì¶ TCI (true color) ready...\")\n",
        "\n",
        "# Upload with progress bar\n",
        "print(f\"\\n‚è≥ Uploading to {upload_url}...\")\n",
        "\n",
        "try:\n",
        "    response = requests.post(upload_url, headers=headers, files=files_to_upload, timeout=300)\n",
        "\n",
        "    # Close file handles\n",
        "    for _, (_, f, _) in files_to_upload.items():\n",
        "        f.close()\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(f\"\\n‚úÖ Upload successful!\")\n",
        "        print(f\"   üìä Status: {result.get('status', 'unknown')}\")\n",
        "\n",
        "        saved_files = result.get('saved', {})\n",
        "        if saved_files:\n",
        "            print(f\"   üìÅ Saved files:\")\n",
        "            for key, path in saved_files.items():\n",
        "                print(f\"      ‚Ä¢ {key}: {path}\")\n",
        "\n",
        "        print(f\"\\nüéâ Your enhanced 1m images are now available in DHARTI!\")\n",
        "        print(f\"   üîÑ Refresh your browser to see the super-resolved layer.\")\n",
        "        print(f\"   üó∫Ô∏è  It will appear as: 'NDVI (S2DR3 1m, AI-enhanced)'\")\n",
        "\n",
        "    elif response.status_code == 403:\n",
        "        print(f\"\\n‚ùå Upload denied: Invalid authentication token\")\n",
        "        print(f\"   This job may have expired or the token is incorrect.\")\n",
        "\n",
        "    elif response.status_code == 404:\n",
        "        print(f\"\\n‚ùå Job not found: {job_id}\")\n",
        "        print(f\"   The job may have been deleted or doesn't exist.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Upload failed: HTTP {response.status_code}\")\n",
        "        print(f\"   Response: {response.text[:200]}\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"\\n‚è±Ô∏è  Upload timeout (>5 minutes)\")\n",
        "    print(f\"   Your files may still be uploading. Check DHARTI status.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Upload error: {e}\")\n",
        "    print(f\"\\nüí° You can manually download the files from /content/clipped/\")\n",
        "    print(f\"   Then contact support to upload them manually.\")"
      ],
      "metadata": {
        "id": "ElG-p4ygZRHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458f051b-c512-4c6f-e893-2028b908976a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Uploading enhanced images to DHARTI...\n",
            "\n",
            "\n",
            "‚è≥ Uploading to https://thirty-windows-add.loca.lt/api/s2dr3/jobs/edc4c4a71d7a4e33a4a32c0bcb0feeb8/upload...\n",
            "\n",
            "‚úÖ Upload successful!\n",
            "   üìä Status: success\n",
            "\n",
            "üéâ Your enhanced 1m images are now available in DHARTI!\n",
            "   üîÑ Refresh your browser to see the super-resolved layer.\n",
            "   üó∫Ô∏è  It will appear as: 'NDVI (S2DR3 1m, AI-enhanced)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Pipeline Complete!\n",
        "\n",
        "The S2DR3 super-resolution has been successfully applied to your satellite imagery!\n",
        "\n",
        "### What was processed:\n",
        "\n",
        "- **Input**: Sentinel-2 10m resolution\n",
        "- **Output**: AI-enhanced 1m resolution (10x improvement!)\n",
        "- **Layers**: RGB (TCI), NDVI, Multi-spectral (10 bands)\n",
        "- **Processing Time**: ~3-5 minutes on GPU\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "1. **Return to DHARTI** web interface\n",
        "2. The modal will **automatically refresh** and show your enhanced images\n",
        "3. Use the **RGB/NDVI toggle** to switch between visualizations\n",
        "4. **Download** high-resolution GeoTIFFs if needed\n",
        "\n",
        "### Technical Details\n",
        "\n",
        "- **Model**: S2DR3 (Sentinel-2 Deep Residual Refine & Restore)\n",
        "- **Resolution**: 10x super-resolution (10m ‚Üí 1m)\n",
        "- **Architecture**: ResNet-based deep learning model\n",
        "- **Output Format**: GeoTIFF with proper georeferencing\n",
        "- **CRS**: EPSG:4326 (WGS84)\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "If you don't see the images in DHARTI:\n",
        "\n",
        "1. Check that the upload completed successfully (green ‚úÖ above)\n",
        "2. Refresh the DHARTI page\n",
        "3. Check browser console for errors\n",
        "4. Verify the job_id matches what you clicked in DHARTI\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Powered by DHARTI + S2DR3**\n",
        "\n",
        "*This automated pipeline brings professional-grade AI super-resolution to agricultural satellite imagery!*\n"
      ],
      "metadata": {
        "id": "UiHkfap8c9R1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7agNXmSpc6V8"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}